---
title: "apple_review_classification"
author: "Rachel Sinondang"
date: "3/10/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Memanggil library

```{r message=FALSE, warning=FALSE}
 # Data Wrangling
library(tidyverse) 

# Text analysis
library(textclean)
library(tidytext)

library(caret)
library(e1071)
library(rpart)
library(RTextTools)
library(tm)
library(DMwR)
```

# Read data
```{r}
apple <- read.csv("apple-twitter-sentiment-texts.csv")
```

# Data Cleansing

```{r}

colSums(is.na(apple))
```

```{r}
count(apple, sentiment)
```

```{r}
apple_clean <- apple %>%
  mutate(ï..text = ï..text %>%
      str_to_lower() %>% # transform menjadi huruf kecil
      replace_url()  %>% 
      replace_html() %>% 
      str_remove_all("@([0-9a-zA-Z_]+)") %>% # remove username
      str_remove_all("#([0-9a-zA-Z_]+)") %>% # remove hashtag
      replace_contraction() %>%
      replace_word_elongation() %>% 
      replace_internet_slang() %>% 
      replace_emoji(.) %>% 
      replace_emoticon(.) %>% 
      str_remove_all(pattern = "[[:digit:]]") %>% # remove number
      str_remove_all(pattern = "[[:punct:]]") %>% 
      str_remove_all(pattern = "%") %>% 
      str_remove_all(pattern = "\\$") %>% # remove dollar sign
      str_remove_all(pattern = "\\|") %>%
      str_remove_all(pattern = "\\=") %>%
      str_remove_all('[\\&]+') %>% 
      str_remove_all('[\\"]+') %>% 
      str_remove_all("<([0-9a-zA-Z_]+)>") %>%
      str_squish(),  # remove extra whitespace
      sentiment = base::factor(sentiment, levels = c("-1", "0", "1"),labels = c("0","1","2"))
  ) %>%
  na.omit() %>% # membuang baris bernilai NA
  as.data.frame() %>% 
  distinct() %>% # hanya keep data yang unik
  mutate(sentiment = as.factor(sentiment))

```


```{r}
# slang_word <- lexicon::hash_internet_slang
add_slang_word <- data.frame(x = c("yall", "needa", "gotta", "ain't", "dammit", "rt", "ima"), y = c("you all", "need", "must", "are not", "damn it", "", "i must admit"))

apple_clean$ï..text <- replace_internet_slang(x = apple_clean$ï..text, slang = add_slang_word$x, replacement = add_slang_word$y)


```

### Text to Corpus

Salah satu package yang bisa digunakan untuk text mining adalah `tm`. Kita akan ubah data teks menjadi corpus dengan function `VCorpus()`.

```{r message=F}
library(tm)

# VCorpus requires a source object, which can be created using VectorSource
apple.corpus <- VCorpus(VectorSource(apple_clean$ï..text))

```

```{r}
# lemmatizing
apple.corpus <- tm_map(apple.corpus, content_transformer(textstem::lemmatize_words))

apple.siap <- bind_cols(apple.corpus %>% sapply(as.character) %>%
  as.data.frame(stringsAsFactors = FALSE), apple_clean[,2]) %>%
  `colnames<-`(c("text","label"))
```

### Splitting data
```{r }
library(rsample)
RNGkind(sample.kind = "Rounding")
set.seed(100)

split_a <- initial_split(apple.siap, prop =  0.6, strata = "label")
data_train <- training(split_a)
data_test <- testing(split_a)

prop.table(table(data_train$label))
```

### Balancing data
```{r}
library(caret)

data_up_train <- upSample(x = data_train %>% select(-label), # data prediktor
                         y = as.factor(data_train$label), # data label
                         yname = "label") # nama kolom label

table(data_up_train$label)
```


### Document-Term Matrix (DTM)
```{r }
apple.train.dtm <- DocumentTermMatrix(VCorpus(VectorSource(data_up_train$text)))

apple.test.dtm <- DocumentTermMatrix(VCorpus(VectorSource(data_test$text)))

# cek data
inspect(apple.train.dtm)
```

```{r}
# ambil terms yang setidaknya muncul di 20 comment
appleFreq <- findFreqTerms(x = apple.train.dtm, lowfreq = 20)
appleFreq
```


```{r }
apple.train.dtm <- removeSparseTerms(apple.train.dtm, 0.995)

```

```{r}
# fungsi DIY
bernoulli_conv <- function(x){
  x <- as.factor(ifelse(x > 0, 1, 0))
  return(x)
}

# coba fungsi
bernoulli_conv(c(0,1,3))
```

Kita terapkan *Bernoulli Converter* ke dalam `data_test` dan `data_train`:

```{r}
data_train_bn <- apply(X = apple.train.dtm, MARGIN = 2, FUN = bernoulli_conv)
data_test_bn <- apply(X = apple.test.dtm, MARGIN = 2, FUN = bernoulli_conv)
```

```{r}
library(e1071)
# your code
model_naive <- naiveBayes(x = data_train_bn, # data prediktor
                          y = data_up_train$label, # data target
                          laplace = 1)


```

```{r}
# your code
apple_predClass <- predict(object = model_naive, 
                         newdata = data_test_bn,
                         type = "class")

head(apple_predClass)
```

```{r}
# your code
library(caret)
confusionMatrix(data = apple_predClass, # hasil prediksi
                reference = data_test$label)
```

### menggabungkan table

untuk membuat model 
```{r}
train_rf <- cbind(as.data.frame(as.matrix(apple.train.dtm)),data_up_train$label)

colnames(train_rf)[394] <- "label"
```

```{r}
library(randomForest)
set.seed(100)

# ctrl <- trainControl(method="repeatedcv", #kfold
#                    number = 6, # berapa k nya
#                     repeats = 3) # 3 kali k fold
# 
# apple_forest <- train(label ~ ., data = train_rf, method = "rf", trControl = ctrl)

# saveRDS(apple_forest, "apple_forest.RDS")

apple_forest <- readRDS("apple_forest.RDS")
```

```{r}

# mengisi feature yang tidak ada di data test menjadi 0
not_include <- apple.train.dtm$dimnames$Terms[!apple.train.dtm$dimnames$Terms %in% apple.test.dtm$dimnames$Terms]

dummy_empty <- train_rf %>% 
              select(not_include) %>% 
              head(1) %>% 
  mutate_all(function(x) 0) # semua diisi 0 karena tidak ada di data test

test_table <- as.data.frame(as.matrix(apple.test.dtm)) %>% 
  bind_cols(dummy_empty)

forest_class <- predict(apple_forest, test_table, type = "raw")
head(forest_class)
```

```{r}
confusionMatrix(data = forest_class, reference = data_test$label)
```



